# Data Processing and Quality Control

## Overview

After data integration, each dataset underwent a comprehensive data processing workflow to ensure consistency, accuracy, and usability. The data processing steps included cleaning, standardizing variables, and performing quality assurance checks. These measures enhance the reliability of analyses conducted using integrated datasets.


## Variable Standardization

To ensure compatibility across data sources, key variables were standardized to maintain consistency in format and structure. 

- **Date Formatting**: Dates were converted to a standard `YYYY-MM-DD` format to enable consistent temporal analyses.
- **Categorical Variables**: Categorical variables such as gender, marital status, and employment type were recoded with consistent labels across datasets. For instance, gender was standardized to “Male” and “Female” for uniformity.
- **Units of Measure**: Variables with measurable units were verified for consistency, ensuring that all values are recorded in the same units.
- **Variable names**: Variable names were standardized by converitng to all lowercase names and replacing spaces with underscores.  This minimizes errors on merges and data joins due to type-case error. The task was done with the help of the "clean_names" function in the Janitor package in R.

## Quality Control Measures

To ensure the reliability and consistency of the processed data, the following quality control measures were implemented:

1. **Cross-Verification with External Data**: The processed data were cross-verified with national statistics to validate accuracy. For example, demographic distributions (e.g., age and gender) were compared against official records.
2. **Internal Consistency Checks**: Consistency checks were conducted to identify and resolve discrepancies within the data. For instance, age and birth year were cross-checked for compatibility.
3. **Random Spot Checks**: A random sample of records was reviewed to ensure data accuracy and completeness across different variables. This included manually reviewing records for logical consistency and comparing raw data with processed output.
4. **Automated Quality Reports**: Automated reports were generated to summarize missing values, outliers, and any inconsistencies found during processing. These reports provided an overview of data quality and highlighted areas that required additional attention.

## Data Transformation Pipeline

The data processing and quality control were organized within a structured pipeline, including the following stages:

1. **Data Import**: Raw data were imported from delivered files in .txt format.
2. **Data Cleaning and Standardization**: Each dataset was cleaned and standardized according to the procedures outlined above.
3. **Linkage and Verification**: After cleaning, datasets were linked by Personal Identity Number, with additional verification steps to confirm accuracy.
4. **Final Quality Control**: A final quality control stage included summary reports and consistency checks, ensuring that the processed data was ready for analysis.

## Dataset Variable Explorer

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
library(DT)
library(readxl)
library(dplyr)

# Load the dataset
metadata <- read_excel("tables/Dataset_variable_lists.xlsx")

# Add tooltips for Variable Description
metadata <- metadata %>%
  mutate(
    `Variable name` = paste0(
      "<span title='", Description, "'>", `Variable name`, "</span>"
    ), 
    `Dataset name` = paste0(    
      "<span title='", `Dataset description`, "'>", `Dataset name`, "</span>"
    ), 
    `Location` = paste0(    
      "<span title='", `Observations in Dataset`," obs", "'>", `Location`, "</span>"
    ), 
  )

# Render the DT table without dropdowns, using regular text-based filters
datatable(
  metadata %>% select( `Variable name`, `Dataset name`, `Source`, `Location`),
  escape = FALSE,  # Allows HTML content in cells for tooltips
  options = list(
    pageLength = 20,              # Number of rows per page
    autoWidth = FALSE,             # Automatically adjust column widths
    searchHighlight = TRUE,       # Highlight search matches
    dom = 'Bfrtip',               # Show buttons for exporting and filtering
    buttons = c('csv', 'excel')   # Allow CSV and Excel export
  ),
  filter = "top"                  # Regular text filter for each column
)

```



